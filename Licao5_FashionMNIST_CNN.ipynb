{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Licao5_FashionMNIST_CNN.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jsansao/dlvc/blob/main/Licao5_FashionMNIST_CNN.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rX8mhOLljYeM"
      },
      "source": [
        "##### Copyright 2019 The TensorFlow Authors.\n",
        "\n",
        "Esse notebook foi adaptado de https://github.com/lmoroney/dlaicourse\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "cellView": "form",
        "id": "BZSlp3DAjdYf"
      },
      "source": [
        "#@title Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        "# you may not use this file except in compliance with the License.\n",
        "# You may obtain a copy of the License at\n",
        "#\n",
        "# https://www.apache.org/licenses/LICENSE-2.0\n",
        "#\n",
        "# Unless required by applicable law or agreed to in writing, software\n",
        "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "# See the License for the specific language governing permissions and\n",
        "# limitations under the License."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R6gHiH-I7uFa"
      },
      "source": [
        "\n",
        "# Melhorando a acúracia usando Convoluções \n",
        "\n",
        "Na lição 3, podemos verificar o desempenho das redes neurais profundas (DNN) com 3 camadas (camada de entrada, camada de saída e camada oculta). \n",
        "\n",
        "Experimentamos com diferentes parâmetros para verificar o desempenho da rede neural para esse conjunto de dados (Fashion MNIST). \n",
        "\n",
        "Abaixo, o código básico da lição para comparação. \n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xcsRtq9OLorS",
        "outputId": "73a4add9-6666-4a8c-ed3d-7a987a04ee76"
      },
      "source": [
        "import tensorflow as tf\n",
        "mnist = tf.keras.datasets.fashion_mnist\n",
        "(training_images, training_labels), (test_images, test_labels) = mnist.load_data()\n",
        "training_images=training_images / 255.0\n",
        "test_images=test_images / 255.0\n",
        "model = tf.keras.models.Sequential([\n",
        "  tf.keras.layers.Flatten(),\n",
        "  tf.keras.layers.Dense(128, activation=tf.nn.relu),\n",
        "  tf.keras.layers.Dense(10, activation=tf.nn.softmax)\n",
        "])\n",
        "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
        "model.fit(training_images, training_labels, epochs=5)\n",
        "\n",
        "test_loss = model.evaluate(test_images, test_labels)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/5\n",
            "1875/1875 [==============================] - 4s 2ms/step - loss: 0.5032 - accuracy: 0.8245\n",
            "Epoch 2/5\n",
            "1875/1875 [==============================] - 3s 2ms/step - loss: 0.3752 - accuracy: 0.8633\n",
            "Epoch 3/5\n",
            "1875/1875 [==============================] - 3s 2ms/step - loss: 0.3381 - accuracy: 0.8761\n",
            "Epoch 4/5\n",
            "1875/1875 [==============================] - 3s 2ms/step - loss: 0.3132 - accuracy: 0.8852\n",
            "Epoch 5/5\n",
            "1875/1875 [==============================] - 3s 2ms/step - loss: 0.2965 - accuracy: 0.8906\n",
            "313/313 [==============================] - 1s 2ms/step - loss: 0.3759 - accuracy: 0.8677\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zldEXSsF8Noz"
      },
      "source": [
        "Note que acúracia de treinamento está na faixa dos 89% e acurácia de validação na faixa de 87%. \n",
        "\n",
        "Com adição de camadas de convolução e pooling antes das camadas densas, formamos a chamada CNN (convolutional neural network). \n",
        "\n",
        "Pelas características de realçamento dos filtros convolucionais, a adição destas camadas tem o potencial de aumentar a acúracia dos classificadores, como veremos a seguir. \n",
        "\n",
        "No trecho a seguir, adicionou-se 2 camadas convolucionais na rede. Compare a acúracia com a rede anterior (totalmente conectada). \n",
        "\n",
        "Para acelerar o processamento, considere alterar o runtime do ambiente para GPU (veja em Runtime/Change Runtime Type/ Hardware Accelerator/GPU).\n",
        "  "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "C0tFgT1MMKi6",
        "outputId": "7c6cd702-a937-43a3-eb04-3d3e02870b3b"
      },
      "source": [
        "import tensorflow as tf\n",
        "print(tf.__version__)\n",
        "mnist = tf.keras.datasets.fashion_mnist\n",
        "(training_images, training_labels), (test_images, test_labels) = mnist.load_data()\n",
        "training_images=training_images.reshape(60000, 28, 28, 1)\n",
        "training_images=training_images / 255.0\n",
        "test_images = test_images.reshape(10000, 28, 28, 1)\n",
        "test_images=test_images/255.0\n",
        "model = tf.keras.models.Sequential([\n",
        "  tf.keras.layers.Conv2D(64, (3,3), activation='relu', input_shape=(28, 28, 1)),\n",
        "  tf.keras.layers.MaxPooling2D(2, 2),\n",
        "  tf.keras.layers.Conv2D(64, (3,3), activation='relu'),\n",
        "  tf.keras.layers.MaxPooling2D(2,2),\n",
        "  tf.keras.layers.Flatten(),\n",
        "  tf.keras.layers.Dense(128, activation='relu'),\n",
        "  tf.keras.layers.Dense(10, activation='softmax')\n",
        "])\n",
        "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
        "model.summary()\n",
        "model.fit(training_images, training_labels, epochs=5)\n",
        "test_loss = model.evaluate(test_images, test_labels)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2.6.0\n",
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/train-labels-idx1-ubyte.gz\n",
            "32768/29515 [=================================] - 0s 0us/step\n",
            "40960/29515 [=========================================] - 0s 0us/step\n",
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/train-images-idx3-ubyte.gz\n",
            "26427392/26421880 [==============================] - 0s 0us/step\n",
            "26435584/26421880 [==============================] - 0s 0us/step\n",
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/t10k-labels-idx1-ubyte.gz\n",
            "16384/5148 [===============================================================================================] - 0s 0us/step\n",
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/t10k-images-idx3-ubyte.gz\n",
            "4423680/4422102 [==============================] - 0s 0us/step\n",
            "4431872/4422102 [==============================] - 0s 0us/step\n",
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "conv2d (Conv2D)              (None, 26, 26, 64)        640       \n",
            "_________________________________________________________________\n",
            "max_pooling2d (MaxPooling2D) (None, 13, 13, 64)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_1 (Conv2D)            (None, 11, 11, 64)        36928     \n",
            "_________________________________________________________________\n",
            "max_pooling2d_1 (MaxPooling2 (None, 5, 5, 64)          0         \n",
            "_________________________________________________________________\n",
            "flatten (Flatten)            (None, 1600)              0         \n",
            "_________________________________________________________________\n",
            "dense (Dense)                (None, 128)               204928    \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              (None, 10)                1290      \n",
            "=================================================================\n",
            "Total params: 243,786\n",
            "Trainable params: 243,786\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Epoch 1/5\n",
            "1875/1875 [==============================] - 36s 3ms/step - loss: 0.4416 - accuracy: 0.8386\n",
            "Epoch 2/5\n",
            "1875/1875 [==============================] - 5s 3ms/step - loss: 0.2973 - accuracy: 0.8911\n",
            "Epoch 3/5\n",
            "1875/1875 [==============================] - 5s 3ms/step - loss: 0.2496 - accuracy: 0.9068\n",
            "Epoch 4/5\n",
            "1875/1875 [==============================] - 5s 3ms/step - loss: 0.2182 - accuracy: 0.9188\n",
            "Epoch 5/5\n",
            "1875/1875 [==============================] - 5s 3ms/step - loss: 0.1934 - accuracy: 0.9269\n",
            "313/313 [==============================] - 1s 3ms/step - loss: 0.2441 - accuracy: 0.9101\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LBFCzrhpFQaX",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a4c7aecb-8cac-4f4e-c355-1d0863d98055"
      },
      "source": [
        "mnist = tf.keras.datasets.fashion_mnist\n",
        "(training_images, training_labels), (test_images, test_labels) = mnist.load_data()\n",
        "training_images2=training_images.reshape(60000, 28, 28, 1)\n",
        "\n",
        "\n",
        "\n",
        "print(training_images.shape)\n",
        "print(training_images2.shape)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(60000, 28, 28)\n",
            "(60000, 28, 28, 1)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uRLfZ0jt-fQI"
      },
      "source": [
        "Note que a acúracia sobe para 93% no conjunto de treinamento e 91% no conjunto de testes. \n",
        "\n",
        "Tente agora rodar por mais épocas (~20), repare o aumento da acúracia de treinamento mas o mesmo não ocorre para a validação. Esse é o fenômeno do *overfitting*.\n",
        "\n",
        "*Overfitting* ocorre quando a rede se especializa excessivamente no conjunto de treinamento, ficando menos efetiva para outros conjuntos de dados. \n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lOUqAiXV5Fbr",
        "outputId": "29076db1-67ce-4523-fc49-ed8f2006f2cf"
      },
      "source": [
        "import tensorflow as tf\n",
        "print(tf.__version__)\n",
        "mnist = tf.keras.datasets.fashion_mnist\n",
        "(training_images, training_labels), (test_images, test_labels) = mnist.load_data()\n",
        "training_images=training_images.reshape(60000, 28, 28, 1)\n",
        "training_images=training_images / 255.0\n",
        "test_images = test_images.reshape(10000, 28, 28, 1)\n",
        "test_images=test_images/255.0\n",
        "model = tf.keras.models.Sequential([\n",
        "  tf.keras.layers.Conv2D(64, (3,3), activation='relu', input_shape=(28, 28, 1)),\n",
        "  tf.keras.layers.MaxPooling2D(2, 2),\n",
        "  tf.keras.layers.Conv2D(64, (3,3), activation='relu'),\n",
        "  tf.keras.layers.MaxPooling2D(2,2),\n",
        "  tf.keras.layers.Flatten(),\n",
        "  tf.keras.layers.Dense(128, activation='relu'),\n",
        "  tf.keras.layers.Dense(10, activation='softmax')\n",
        "])\n",
        "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
        "model.summary()\n",
        "model.fit(training_images, training_labels, epochs=20)\n",
        "test_loss = model.evaluate(test_images, test_labels)\n"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2.6.0\n",
            "Model: \"sequential_5\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "conv2d_8 (Conv2D)            (None, 26, 26, 64)        640       \n",
            "_________________________________________________________________\n",
            "max_pooling2d_8 (MaxPooling2 (None, 13, 13, 64)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_9 (Conv2D)            (None, 11, 11, 64)        36928     \n",
            "_________________________________________________________________\n",
            "max_pooling2d_9 (MaxPooling2 (None, 5, 5, 64)          0         \n",
            "_________________________________________________________________\n",
            "flatten_5 (Flatten)          (None, 1600)              0         \n",
            "_________________________________________________________________\n",
            "dense_10 (Dense)             (None, 128)               204928    \n",
            "_________________________________________________________________\n",
            "dense_11 (Dense)             (None, 10)                1290      \n",
            "=================================================================\n",
            "Total params: 243,786\n",
            "Trainable params: 243,786\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Epoch 1/20\n",
            "1875/1875 [==============================] - 5s 3ms/step - loss: 0.4447 - accuracy: 0.8404\n",
            "Epoch 2/20\n",
            "1875/1875 [==============================] - 5s 3ms/step - loss: 0.2954 - accuracy: 0.8921\n",
            "Epoch 3/20\n",
            "1875/1875 [==============================] - 5s 3ms/step - loss: 0.2472 - accuracy: 0.9089\n",
            "Epoch 4/20\n",
            "1875/1875 [==============================] - 5s 3ms/step - loss: 0.2141 - accuracy: 0.9200\n",
            "Epoch 5/20\n",
            "1875/1875 [==============================] - 5s 3ms/step - loss: 0.1909 - accuracy: 0.9279\n",
            "Epoch 6/20\n",
            "1875/1875 [==============================] - 5s 3ms/step - loss: 0.1655 - accuracy: 0.9375\n",
            "Epoch 7/20\n",
            "1875/1875 [==============================] - 5s 3ms/step - loss: 0.1460 - accuracy: 0.9453\n",
            "Epoch 8/20\n",
            "1875/1875 [==============================] - 5s 3ms/step - loss: 0.1286 - accuracy: 0.9510\n",
            "Epoch 9/20\n",
            "1875/1875 [==============================] - 5s 3ms/step - loss: 0.1147 - accuracy: 0.9565\n",
            "Epoch 10/20\n",
            "1875/1875 [==============================] - 5s 3ms/step - loss: 0.0983 - accuracy: 0.9621\n",
            "Epoch 11/20\n",
            "1875/1875 [==============================] - 5s 3ms/step - loss: 0.0887 - accuracy: 0.9664\n",
            "Epoch 12/20\n",
            "1875/1875 [==============================] - 5s 3ms/step - loss: 0.0779 - accuracy: 0.9704\n",
            "Epoch 13/20\n",
            "1875/1875 [==============================] - 5s 3ms/step - loss: 0.0707 - accuracy: 0.9732\n",
            "Epoch 14/20\n",
            "1875/1875 [==============================] - 5s 3ms/step - loss: 0.0599 - accuracy: 0.9767\n",
            "Epoch 15/20\n",
            "1875/1875 [==============================] - 5s 3ms/step - loss: 0.0573 - accuracy: 0.9781\n",
            "Epoch 16/20\n",
            "1875/1875 [==============================] - 5s 3ms/step - loss: 0.0526 - accuracy: 0.9801\n",
            "Epoch 17/20\n",
            "1875/1875 [==============================] - 5s 3ms/step - loss: 0.0477 - accuracy: 0.9823\n",
            "Epoch 18/20\n",
            "1875/1875 [==============================] - 5s 3ms/step - loss: 0.0422 - accuracy: 0.9843\n",
            "Epoch 19/20\n",
            "1875/1875 [==============================] - 5s 3ms/step - loss: 0.0428 - accuracy: 0.9837\n",
            "Epoch 20/20\n",
            "1875/1875 [==============================] - 5s 3ms/step - loss: 0.0388 - accuracy: 0.9855\n",
            "313/313 [==============================] - 1s 2ms/step - loss: 0.5155 - accuracy: 0.9124\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RaLX5cgI_JDb"
      },
      "source": [
        "\n",
        "Note que o primeiro passo é transformar os dados de entrada em um tensor. Originalmente, tem-se uma lista de 60000 itens com 28x28x1, passa-se a um tensor 60000x28x28x1. \n",
        "\n",
        "O passo é necessário para o uso da camada de convolução. \n",
        "\n",
        "```\n",
        "import tensorflow as tf\n",
        "mnist = tf.keras.datasets.fashion_mnist\n",
        "(training_images, training_labels), (test_images, test_labels) = mnist.load_data()\n",
        "training_images=training_images.reshape(60000, 28, 28, 1)\n",
        "training_images=training_images / 255.0\n",
        "test_images = test_images.reshape(10000, 28, 28, 1)\n",
        "test_images=test_images/255.0\n",
        "```\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SS_W_INc_kJQ"
      },
      "source": [
        "Próximo passo é definir o modelo. \n",
        "\n",
        "Na camada inicial convolucional:\n",
        "\n",
        "1. O número de convoluções da primeira camada é puramente arbitrário. Um bom palpite inicial é 32. \n",
        "\n",
        "2. Tamanho da matriz de convolução (por exemplo 3x3)\n",
        "\n",
        "3. Função de ativação (neste caso reLu)\n",
        "\n",
        "4. Formato de entrada: neste caso (28,28,1), imagens 28x28 em tons de cinza (uma dimensão)\n",
        "\n",
        "O próximo passo é seguir a camada de convolução por uma camada de \"MaxPooling\", que tem como objetivo comprimir a imagem filtrada, isto é, diminuir o número de parâmetros entre uma etapa e outra. \n",
        "\n",
        "Ao especificar  (2,2) no Pooling, o tamanho vai para um quarto do original. Dividindo a imagem total em arranjos 2x2 de pixels, o método escolhe 1 dos 4 pixels, o de maior módulo. \n",
        "\n",
        "O comando ``model.summary()`` descreve o tamanho e forma da rede neural. A cada camada de pooling, o tamanho da imagem é reduzido. \n",
        "\n",
        "\n",
        "```\n",
        "model = tf.keras.models.Sequential([\n",
        "  tf.keras.layers.Conv2D(32, (3,3), activation='relu', input_shape=(28, 28, 1)),\n",
        "  tf.keras.layers.MaxPooling2D(2, 2),\n",
        "```\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RMorM6daADjA"
      },
      "source": [
        "Adicionando mais uma camada de convulação/pooling\n",
        "\n",
        "\n",
        "```\n",
        "  tf.keras.layers.Conv2D(64, (3,3), activation='relu'),\n",
        "  tf.keras.layers.MaxPooling2D(2,2)\n",
        "```\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b1-x-kZF4_tC"
      },
      "source": [
        "Achatando a saída, para se ter o mesmo tipo de saída da rede convencional\n",
        "\n",
        "```\n",
        "  tf.keras.layers.Flatten(),\n",
        "```\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qPtqR23uASjX"
      },
      "source": [
        "\n",
        "\n",
        "Aqui, colocou-se 128 neurônios na camada densa e 10 neurônios na camada de saída. \n",
        "\n",
        "\n",
        "```\n",
        "  tf.keras.layers.Dense(128, activation='relu'),\n",
        "  tf.keras.layers.Dense(10, activation='softmax')\n",
        "])\n",
        "```\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C0GSsjUhAaSj"
      },
      "source": [
        "\n",
        "\n",
        "Finalmente a compilação, treinamento e avaliação do modelo. \n",
        "\n",
        "\n",
        "```\n",
        "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
        "model.fit(training_images, training_labels, epochs=5)\n",
        "test_loss, test_acc = model.evaluate(test_images, test_labels)\n",
        "print(test_acc)\n",
        "```\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IXx_LX3SAlFs"
      },
      "source": [
        "# Visualizing the Convolutions and Pooling\n",
        "\n",
        "This code will show us the convolutions graphically. The print (test_labels[;100]) shows us the first 100 labels in the test set, and you can see that the ones at index 0, index 23 and index 28 are all the same value (9). They're all shoes. Let's take a look at the result of running the convolution on each, and you'll begin to see common features between them emerge. Now, when the DNN is training on that data, it's working with a lot less, and it's perhaps finding a commonality between shoes based on this convolution/pooling combination."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        },
        "id": "f-6nX4QsOku6",
        "outputId": "6b85ed93-6868-4c2c-b066-0808d6536878"
      },
      "source": [
        "print(test_labels[:100])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[9 2 1 1 6 1 4 6 5 7 4 5 7 3 4 1 2 4 8 0 2 5 7 9 1 4 6 0 9 3 8 8 3 3 8 0 7\n",
            " 5 7 9 6 1 3 7 6 7 2 1 2 2 4 4 5 8 2 2 8 4 8 0 7 7 8 5 1 1 2 3 9 8 7 0 2 6\n",
            " 2 3 1 2 8 4 1 8 5 9 5 0 3 2 0 6 5 3 6 7 1 8 0 1 4 2]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 268
        },
        "id": "9FGsHhv6JvDx",
        "outputId": "31407b9b-b837-43d0-efb4-26289689c546"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "f, axarr = plt.subplots(3,4)\n",
        "#0,23,28 - bota\n",
        "#2,3,5 - calça\n",
        "FIRST_IMAGE=0\n",
        "SECOND_IMAGE=23\n",
        "THIRD_IMAGE=28\n",
        "CONVOLUTION_NUMBER = 1\n",
        "from tensorflow.keras import models\n",
        "layer_outputs = [layer.output for layer in model.layers]\n",
        "activation_model = tf.keras.models.Model(inputs = model.input, outputs = layer_outputs)\n",
        "for x in range(0,4):\n",
        "  f1 = activation_model.predict(test_images[FIRST_IMAGE].reshape(1, 28, 28, 1))[x]\n",
        "  axarr[0,x].imshow(f1[0, : , :, CONVOLUTION_NUMBER], cmap='inferno')\n",
        "  axarr[0,x].grid(False)\n",
        "  f2 = activation_model.predict(test_images[SECOND_IMAGE].reshape(1, 28, 28, 1))[x]\n",
        "  axarr[1,x].imshow(f2[0, : , :, CONVOLUTION_NUMBER], cmap='inferno')\n",
        "  axarr[1,x].grid(False)\n",
        "  f3 = activation_model.predict(test_images[THIRD_IMAGE].reshape(1, 28, 28, 1))[x]\n",
        "  axarr[2,x].imshow(f3[0, : , :, CONVOLUTION_NUMBER], cmap='inferno')\n",
        "  axarr[2,x].grid(False)"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 12 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8KVPZqgHo5Ux"
      },
      "source": [
        "## Exercícios exploratórios\n",
        "\n",
        "\n",
        "1. Varie o número de convoluções. Mude N=32 para 16 ou 64. Qual é o impacto na acurácia e tempo de treinamento?\n",
        "\n",
        "N=32\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZpYRidBXpBPM",
        "outputId": "5565a75d-edcd-44a4-8f88-c537ad43294b"
      },
      "source": [
        "import tensorflow as tf\n",
        "print(tf.__version__)\n",
        "mnist = tf.keras.datasets.fashion_mnist\n",
        "(training_images, training_labels), (test_images, test_labels) = mnist.load_data()\n",
        "training_images=training_images.reshape(60000, 28, 28, 1)\n",
        "training_images=training_images / 255.0\n",
        "test_images = test_images.reshape(10000, 28, 28, 1)\n",
        "test_images=test_images/255.0\n",
        "model = tf.keras.models.Sequential([\n",
        "  tf.keras.layers.Conv2D(32, (3,3), activation='relu', input_shape=(28, 28, 1)),\n",
        "  tf.keras.layers.MaxPooling2D(2, 2),\n",
        "  tf.keras.layers.Conv2D(32, (3,3), activation='relu'),\n",
        "  tf.keras.layers.MaxPooling2D(2,2),  \n",
        "  tf.keras.layers.Flatten(),\n",
        "  tf.keras.layers.Dense(128, activation='relu'),\n",
        "  tf.keras.layers.Dense(10, activation='softmax')\n",
        "])\n",
        "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
        "model.fit(training_images, training_labels, epochs=10)\n",
        "test_loss, test_acc = model.evaluate(test_images, test_labels)\n",
        "print(test_acc)"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2.6.0\n",
            "Epoch 1/10\n",
            "1875/1875 [==============================] - 5s 3ms/step - loss: 0.4688 - accuracy: 0.8294\n",
            "Epoch 2/10\n",
            "1875/1875 [==============================] - 5s 3ms/step - loss: 0.3166 - accuracy: 0.8836\n",
            "Epoch 3/10\n",
            "1875/1875 [==============================] - 5s 3ms/step - loss: 0.2705 - accuracy: 0.9004\n",
            "Epoch 4/10\n",
            "1875/1875 [==============================] - 5s 3ms/step - loss: 0.2416 - accuracy: 0.9105\n",
            "Epoch 5/10\n",
            "1875/1875 [==============================] - 5s 3ms/step - loss: 0.2192 - accuracy: 0.9173\n",
            "Epoch 6/10\n",
            "1875/1875 [==============================] - 5s 3ms/step - loss: 0.2000 - accuracy: 0.9265\n",
            "Epoch 7/10\n",
            "1875/1875 [==============================] - 5s 3ms/step - loss: 0.1808 - accuracy: 0.9320\n",
            "Epoch 8/10\n",
            "1875/1875 [==============================] - 5s 3ms/step - loss: 0.1638 - accuracy: 0.9384\n",
            "Epoch 9/10\n",
            "1875/1875 [==============================] - 5s 3ms/step - loss: 0.1485 - accuracy: 0.9447\n",
            "Epoch 10/10\n",
            "1875/1875 [==============================] - 5s 3ms/step - loss: 0.1357 - accuracy: 0.9493\n",
            "313/313 [==============================] - 1s 2ms/step - loss: 0.2796 - accuracy: 0.9133\n",
            "0.9132999777793884\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9pX0oZBcXPV8"
      },
      "source": [
        "N=16"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ETyQ9U7qXYoA",
        "outputId": "db33fd5b-0e45-41f3-9d7a-8562da1fcde6"
      },
      "source": [
        "import tensorflow as tf\n",
        "print(tf.__version__)\n",
        "mnist = tf.keras.datasets.fashion_mnist\n",
        "(training_images, training_labels), (test_images, test_labels) = mnist.load_data()\n",
        "training_images=training_images.reshape(60000, 28, 28, 1)\n",
        "training_images=training_images / 255.0\n",
        "test_images = test_images.reshape(10000, 28, 28, 1)\n",
        "test_images=test_images/255.0\n",
        "model = tf.keras.models.Sequential([\n",
        "  tf.keras.layers.Conv2D(16, (3,3), activation='relu', input_shape=(28, 28, 1)),\n",
        "  tf.keras.layers.MaxPooling2D(2, 2),\n",
        "  tf.keras.layers.Conv2D(16, (3,3), activation='relu'),\n",
        "  tf.keras.layers.MaxPooling2D(2,2),  \n",
        "  tf.keras.layers.Flatten(),\n",
        "  tf.keras.layers.Dense(128, activation='relu'),\n",
        "  tf.keras.layers.Dense(10, activation='softmax')\n",
        "])\n",
        "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
        "model.fit(training_images, training_labels, epochs=10)\n",
        "test_loss, test_acc = model.evaluate(test_images, test_labels)\n",
        "print(test_acc)"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2.6.0\n",
            "Epoch 1/10\n",
            "1875/1875 [==============================] - 5s 2ms/step - loss: 0.5087 - accuracy: 0.8153\n",
            "Epoch 2/10\n",
            "1875/1875 [==============================] - 4s 2ms/step - loss: 0.3539 - accuracy: 0.8698\n",
            "Epoch 3/10\n",
            "1875/1875 [==============================] - 4s 2ms/step - loss: 0.3102 - accuracy: 0.8857\n",
            "Epoch 4/10\n",
            "1875/1875 [==============================] - 4s 2ms/step - loss: 0.2814 - accuracy: 0.8960\n",
            "Epoch 5/10\n",
            "1875/1875 [==============================] - 5s 2ms/step - loss: 0.2573 - accuracy: 0.9047\n",
            "Epoch 6/10\n",
            "1875/1875 [==============================] - 4s 2ms/step - loss: 0.2395 - accuracy: 0.9105\n",
            "Epoch 7/10\n",
            "1875/1875 [==============================] - 5s 2ms/step - loss: 0.2249 - accuracy: 0.9154\n",
            "Epoch 8/10\n",
            "1875/1875 [==============================] - 4s 2ms/step - loss: 0.2104 - accuracy: 0.9213\n",
            "Epoch 9/10\n",
            "1875/1875 [==============================] - 4s 2ms/step - loss: 0.1971 - accuracy: 0.9264\n",
            "Epoch 10/10\n",
            "1875/1875 [==============================] - 4s 2ms/step - loss: 0.1878 - accuracy: 0.9301\n",
            "313/313 [==============================] - 1s 2ms/step - loss: 0.2683 - accuracy: 0.9058\n",
            "0.9057999849319458\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8I3_tz_IXSI4"
      },
      "source": [
        "N=64"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HTe-_ylbUHjZ",
        "outputId": "16f12d91-1fa6-4826-f49a-8e6f847de4a0"
      },
      "source": [
        "import tensorflow as tf\n",
        "print(tf.__version__)\n",
        "mnist = tf.keras.datasets.fashion_mnist\n",
        "(training_images, training_labels), (test_images, test_labels) = mnist.load_data()\n",
        "training_images=training_images.reshape(60000, 28, 28, 1)\n",
        "training_images=training_images / 255.0\n",
        "test_images = test_images.reshape(10000, 28, 28, 1)\n",
        "test_images=test_images/255.0\n",
        "model = tf.keras.models.Sequential([\n",
        "  tf.keras.layers.Conv2D(64, (3,3), activation='relu', input_shape=(28, 28, 1)),\n",
        "  tf.keras.layers.MaxPooling2D(2, 2),\n",
        "  tf.keras.layers.Conv2D(64, (3,3), activation='relu'),\n",
        "  tf.keras.layers.MaxPooling2D(2,2),  \n",
        "  tf.keras.layers.Flatten(),\n",
        "  tf.keras.layers.Dense(128, activation='relu'),\n",
        "  tf.keras.layers.Dense(10, activation='softmax')\n",
        "])\n",
        "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
        "model.fit(training_images, training_labels, epochs=10)\n",
        "test_loss, test_acc = model.evaluate(test_images, test_labels)\n",
        "print(test_acc)"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2.6.0\n",
            "Epoch 1/10\n",
            "1875/1875 [==============================] - 5s 3ms/step - loss: 0.4375 - accuracy: 0.8401\n",
            "Epoch 2/10\n",
            "1875/1875 [==============================] - 5s 3ms/step - loss: 0.2884 - accuracy: 0.8948\n",
            "Epoch 3/10\n",
            "1875/1875 [==============================] - 5s 3ms/step - loss: 0.2430 - accuracy: 0.9096\n",
            "Epoch 4/10\n",
            "1875/1875 [==============================] - 5s 3ms/step - loss: 0.2143 - accuracy: 0.9205\n",
            "Epoch 5/10\n",
            "1875/1875 [==============================] - 5s 3ms/step - loss: 0.1857 - accuracy: 0.9312\n",
            "Epoch 6/10\n",
            "1875/1875 [==============================] - 5s 3ms/step - loss: 0.1643 - accuracy: 0.9389\n",
            "Epoch 7/10\n",
            "1875/1875 [==============================] - 5s 3ms/step - loss: 0.1436 - accuracy: 0.9468\n",
            "Epoch 8/10\n",
            "1875/1875 [==============================] - 5s 3ms/step - loss: 0.1260 - accuracy: 0.9528\n",
            "Epoch 9/10\n",
            "1875/1875 [==============================] - 5s 3ms/step - loss: 0.1093 - accuracy: 0.9589\n",
            "Epoch 10/10\n",
            "1875/1875 [==============================] - 5s 3ms/step - loss: 0.0977 - accuracy: 0.9630\n",
            "313/313 [==============================] - 1s 2ms/step - loss: 0.2917 - accuracy: 0.9123\n",
            "0.9122999906539917\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9TclRWgHT6_B"
      },
      "source": [
        "\n",
        "2. Remova a última convolução. Qual é o impacto?\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YcjA34wzXvqe",
        "outputId": "de0f328f-3c2b-4002-93a3-e94c53ed87b5"
      },
      "source": [
        "import tensorflow as tf\n",
        "print(tf.__version__)\n",
        "mnist = tf.keras.datasets.fashion_mnist\n",
        "(training_images, training_labels), (test_images, test_labels) = mnist.load_data()\n",
        "training_images=training_images.reshape(60000, 28, 28, 1)\n",
        "training_images=training_images / 255.0\n",
        "test_images = test_images.reshape(10000, 28, 28, 1)\n",
        "test_images=test_images/255.0\n",
        "model = tf.keras.models.Sequential([\n",
        "  tf.keras.layers.Conv2D(32, (3,3), activation='relu', input_shape=(28, 28, 1)),\n",
        "  tf.keras.layers.MaxPooling2D(2, 2),\n",
        "  #tf.keras.layers.Conv2D(32, (3,3), activation='relu'),\n",
        "  #tf.keras.layers.MaxPooling2D(2,2),  \n",
        "  tf.keras.layers.Flatten(),\n",
        "  tf.keras.layers.Dense(128, activation='relu'),\n",
        "  tf.keras.layers.Dense(10, activation='softmax')\n",
        "])\n",
        "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
        "model.fit(training_images, training_labels, epochs=10)\n",
        "test_loss, test_acc = model.evaluate(test_images, test_labels)\n",
        "print(test_acc)"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2.6.0\n",
            "Epoch 1/10\n",
            "1875/1875 [==============================] - 5s 2ms/step - loss: 0.3934 - accuracy: 0.8601\n",
            "Epoch 2/10\n",
            "1875/1875 [==============================] - 4s 2ms/step - loss: 0.2695 - accuracy: 0.9028\n",
            "Epoch 3/10\n",
            "1875/1875 [==============================] - 4s 2ms/step - loss: 0.2272 - accuracy: 0.9160\n",
            "Epoch 4/10\n",
            "1875/1875 [==============================] - 4s 2ms/step - loss: 0.1924 - accuracy: 0.9284\n",
            "Epoch 5/10\n",
            "1875/1875 [==============================] - 4s 2ms/step - loss: 0.1665 - accuracy: 0.9388\n",
            "Epoch 6/10\n",
            "1875/1875 [==============================] - 4s 2ms/step - loss: 0.1421 - accuracy: 0.9477\n",
            "Epoch 7/10\n",
            "1875/1875 [==============================] - 4s 2ms/step - loss: 0.1219 - accuracy: 0.9545\n",
            "Epoch 8/10\n",
            "1875/1875 [==============================] - 4s 2ms/step - loss: 0.1033 - accuracy: 0.9616\n",
            "Epoch 9/10\n",
            "1875/1875 [==============================] - 4s 2ms/step - loss: 0.0881 - accuracy: 0.9682\n",
            "Epoch 10/10\n",
            "1875/1875 [==============================] - 4s 2ms/step - loss: 0.0769 - accuracy: 0.9719\n",
            "313/313 [==============================] - 1s 2ms/step - loss: 0.3205 - accuracy: 0.9149\n",
            "0.914900004863739\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "chcQaKAvXyO0"
      },
      "source": [
        "\n",
        "3. Adicione mais uma camada convolucional. Qual é o impacto?\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vfNp_bPQYTrO",
        "outputId": "8d822f00-b952-47ed-a8ec-28c7b82891ce"
      },
      "source": [
        "import tensorflow as tf\n",
        "print(tf.__version__)\n",
        "mnist = tf.keras.datasets.fashion_mnist\n",
        "(training_images, training_labels), (test_images, test_labels) = mnist.load_data()\n",
        "training_images=training_images.reshape(60000, 28, 28, 1)\n",
        "training_images=training_images / 255.0\n",
        "test_images = test_images.reshape(10000, 28, 28, 1)\n",
        "test_images=test_images/255.0\n",
        "model = tf.keras.models.Sequential([\n",
        "  tf.keras.layers.Conv2D(32, (3,3), activation='relu', input_shape=(28, 28, 1)),\n",
        "  tf.keras.layers.MaxPooling2D(2, 2),\n",
        "  tf.keras.layers.Conv2D(32, (3,3), activation='relu'),\n",
        "  tf.keras.layers.MaxPooling2D(2,2),  \n",
        "  tf.keras.layers.Conv2D(32, (3,3), activation='relu'),\n",
        "  tf.keras.layers.MaxPooling2D(2,2),  \n",
        "  tf.keras.layers.Flatten(),\n",
        "  tf.keras.layers.Dense(128, activation='relu'),\n",
        "  tf.keras.layers.Dense(10, activation='softmax')\n",
        "])\n",
        "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
        "model.fit(training_images, training_labels, epochs=10)\n",
        "test_loss, test_acc = model.evaluate(test_images, test_labels)\n",
        "print(test_acc)"
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2.6.0\n",
            "Epoch 1/10\n",
            "1875/1875 [==============================] - 6s 3ms/step - loss: 0.6282 - accuracy: 0.7709\n",
            "Epoch 2/10\n",
            "1875/1875 [==============================] - 5s 3ms/step - loss: 0.4314 - accuracy: 0.8416\n",
            "Epoch 3/10\n",
            "1875/1875 [==============================] - 5s 3ms/step - loss: 0.3803 - accuracy: 0.8607\n",
            "Epoch 4/10\n",
            "1875/1875 [==============================] - 5s 3ms/step - loss: 0.3462 - accuracy: 0.8723\n",
            "Epoch 5/10\n",
            "1875/1875 [==============================] - 5s 3ms/step - loss: 0.3201 - accuracy: 0.8827\n",
            "Epoch 6/10\n",
            "1875/1875 [==============================] - 5s 3ms/step - loss: 0.3004 - accuracy: 0.8889\n",
            "Epoch 7/10\n",
            "1875/1875 [==============================] - 5s 3ms/step - loss: 0.2830 - accuracy: 0.8956\n",
            "Epoch 8/10\n",
            "1875/1875 [==============================] - 5s 3ms/step - loss: 0.2681 - accuracy: 0.9005\n",
            "Epoch 9/10\n",
            "1875/1875 [==============================] - 5s 3ms/step - loss: 0.2576 - accuracy: 0.9041\n",
            "Epoch 10/10\n",
            "1875/1875 [==============================] - 5s 3ms/step - loss: 0.2472 - accuracy: 0.9078\n",
            "313/313 [==============================] - 1s 2ms/step - loss: 0.3400 - accuracy: 0.8772\n",
            "0.8772000074386597\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i9v2M_mKXzfi"
      },
      "source": [
        "\n",
        "4. Implemente o callback para interromper o treinamento com um determinado nível de acurácia ou perda. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "U773sDQdX3NZ",
        "outputId": "160f7479-7d24-428d-97fa-382ccc6092c8"
      },
      "source": [
        "import tensorflow as tf\n",
        "print(tf.__version__)\n",
        "\n",
        "\n",
        "class myCallback(tf.keras.callbacks.Callback):\n",
        "  def on_epoch_end(self, epoch, logs={}):\n",
        "    if(logs.get('loss')<0.4):\n",
        "      print(\"\\nAtingiu loss < 0.4, cancelando treinamento\")\n",
        "      self.model.stop_training = True\n",
        "\n",
        "callbacks = myCallback()\n",
        "\n",
        "mnist = tf.keras.datasets.fashion_mnist\n",
        "(training_images, training_labels), (test_images, test_labels) = mnist.load_data()\n",
        "training_images=training_images.reshape(60000, 28, 28, 1)\n",
        "training_images=training_images / 255.0\n",
        "test_images = test_images.reshape(10000, 28, 28, 1)\n",
        "test_images=test_images/255.0\n",
        "model = tf.keras.models.Sequential([\n",
        "  tf.keras.layers.Conv2D(16, (3,3), activation='relu', input_shape=(28, 28, 1)),\n",
        "  tf.keras.layers.MaxPooling2D(2, 2),\n",
        "  tf.keras.layers.Conv2D(16, (3,3), activation='relu'),\n",
        "  tf.keras.layers.MaxPooling2D(2,2),  \n",
        "  tf.keras.layers.Flatten(),\n",
        "  tf.keras.layers.Dense(128, activation='relu'),\n",
        "  tf.keras.layers.Dense(10, activation='softmax')\n",
        "])\n",
        "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
        "model.fit(training_images, training_labels, epochs=10, callbacks=[callbacks])\n",
        "test_loss, test_acc = model.evaluate(test_images, test_labels)\n",
        "print(test_acc)"
      ],
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2.6.0\n",
            "Epoch 1/10\n",
            "1875/1875 [==============================] - 5s 2ms/step - loss: 0.5359 - accuracy: 0.8053\n",
            "Epoch 2/10\n",
            "1875/1875 [==============================] - 5s 2ms/step - loss: 0.3562 - accuracy: 0.8720\n",
            "\n",
            "Atingiu loss < 0.4, cancelando treinamento\n",
            "313/313 [==============================] - 1s 2ms/step - loss: 0.3492 - accuracy: 0.8740\n",
            "0.8740000128746033\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}