{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Licao9bis_TFD_ExpressaoFacial.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyPu1/w0e8soRyELpeUka25v",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jsansao/dlvc/blob/main/Licao9bis_TFD_ExpressaoFacial.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Wx0NSqINcFg6",
        "outputId": "a1119521-5c88-4927-c070-853456928605"
      },
      "source": [
        "import numpy as np\n",
        "\n",
        "\n",
        "!wget https://github.com/tauster/Facial-Expression-Classifier/raw/master/toronto_face.npz\n",
        "\n",
        "data = np.load('toronto_face.npz') \n",
        "\n",
        "X_train = data['inputs_train'].reshape(-1,48,48)\n",
        "X_valid = data['inputs_valid'].reshape(-1,48,48)\n",
        "X_test = data['inputs_test'].reshape(-1,48,48)\n",
        "\n",
        "Y_train = data['target_train']\n",
        "Y_valid = data['target_valid']\n",
        "Y_test = data['target_test']\n",
        "\n",
        "data.files\n",
        "\n"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2021-11-24 16:32:08--  https://github.com/tauster/Facial-Expression-Classifier/raw/master/toronto_face.npz\n",
            "Resolving github.com (github.com)... 140.82.113.3\n",
            "Connecting to github.com (github.com)|140.82.113.3|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://raw.githubusercontent.com/tauster/Facial-Expression-Classifier/master/toronto_face.npz [following]\n",
            "--2021-11-24 16:32:08--  https://raw.githubusercontent.com/tauster/Facial-Expression-Classifier/master/toronto_face.npz\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 12375556 (12M) [application/octet-stream]\n",
            "Saving to: ‘toronto_face.npz’\n",
            "\n",
            "toronto_face.npz    100%[===================>]  11.80M  --.-KB/s    in 0.1s    \n",
            "\n",
            "2021-11-24 16:32:09 (107 MB/s) - ‘toronto_face.npz’ saved [12375556/12375556]\n",
            "\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['target_valid',\n",
              " 'target_train',\n",
              " 'target_test',\n",
              " 'inputs_train',\n",
              " 'inputs_test',\n",
              " 'inputs_valid']"
            ]
          },
          "metadata": {},
          "execution_count": 1
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2_9kQ5tlgKMz",
        "outputId": "27a80197-7fff-49b9-b801-fca5e7a060b3"
      },
      "source": [
        "import tensorflow as tf\n",
        "print(tf.__version__)\n",
        "\n",
        "X_train = X_train / 255.0\n",
        " \n",
        "X_valid= X_valid /255.0"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2.7.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "g607es4CegDl",
        "outputId": "432bbf7b-854e-495a-90d4-8527deab39c1"
      },
      "source": [
        "\n",
        "model = tf.keras.models.Sequential([\n",
        "  tf.keras.layers.Conv2D(64, (3,3), activation='relu', input_shape=(48, 48, 1)),\n",
        "  tf.keras.layers.MaxPooling2D(2, 2),\n",
        "  tf.keras.layers.Conv2D(64, (3,3), activation='relu'),\n",
        "  tf.keras.layers.MaxPooling2D(2,2),\n",
        "  tf.keras.layers.Flatten(),\n",
        "  tf.keras.layers.Dense(128, activation='relu'),\n",
        "  tf.keras.layers.Dense(7, activation='softmax')\n",
        "])\n",
        "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
        "model.summary()\n",
        "model.fit(X_train, Y_train, validation_data=(X_valid, Y_valid), epochs=20)\n",
        "test_loss = model.evaluate(X_valid, Y_valid)"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential_1\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " conv2d_2 (Conv2D)           (None, 46, 46, 64)        640       \n",
            "                                                                 \n",
            " max_pooling2d_2 (MaxPooling  (None, 23, 23, 64)       0         \n",
            " 2D)                                                             \n",
            "                                                                 \n",
            " conv2d_3 (Conv2D)           (None, 21, 21, 64)        36928     \n",
            "                                                                 \n",
            " max_pooling2d_3 (MaxPooling  (None, 10, 10, 64)       0         \n",
            " 2D)                                                             \n",
            "                                                                 \n",
            " flatten_1 (Flatten)         (None, 6400)              0         \n",
            "                                                                 \n",
            " dense_2 (Dense)             (None, 128)               819328    \n",
            "                                                                 \n",
            " dense_3 (Dense)             (None, 7)                 903       \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 857,799\n",
            "Trainable params: 857,799\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Epoch 1/20\n",
            "106/106 [==============================] - 17s 157ms/step - loss: 1.6694 - accuracy: 0.3809 - val_loss: 1.2878 - val_accuracy: 0.5346\n",
            "Epoch 2/20\n",
            "106/106 [==============================] - 17s 159ms/step - loss: 1.1119 - accuracy: 0.6067 - val_loss: 0.9845 - val_accuracy: 0.6372\n",
            "Epoch 3/20\n",
            "106/106 [==============================] - 16s 155ms/step - loss: 0.8859 - accuracy: 0.6915 - val_loss: 0.7626 - val_accuracy: 0.7255\n",
            "Epoch 4/20\n",
            "106/106 [==============================] - 16s 155ms/step - loss: 0.7456 - accuracy: 0.7386 - val_loss: 0.7014 - val_accuracy: 0.7494\n",
            "Epoch 5/20\n",
            "106/106 [==============================] - 16s 155ms/step - loss: 0.6137 - accuracy: 0.7851 - val_loss: 0.6771 - val_accuracy: 0.7733\n",
            "Epoch 6/20\n",
            "106/106 [==============================] - 17s 157ms/step - loss: 0.5441 - accuracy: 0.8124 - val_loss: 0.6670 - val_accuracy: 0.7661\n",
            "Epoch 7/20\n",
            "106/106 [==============================] - 17s 161ms/step - loss: 0.4674 - accuracy: 0.8382 - val_loss: 0.5908 - val_accuracy: 0.7733\n",
            "Epoch 8/20\n",
            "106/106 [==============================] - 17s 164ms/step - loss: 0.4141 - accuracy: 0.8500 - val_loss: 0.6093 - val_accuracy: 0.7709\n",
            "Epoch 9/20\n",
            "106/106 [==============================] - 17s 157ms/step - loss: 0.3479 - accuracy: 0.8785 - val_loss: 0.6471 - val_accuracy: 0.7709\n",
            "Epoch 10/20\n",
            "106/106 [==============================] - 17s 159ms/step - loss: 0.2920 - accuracy: 0.8936 - val_loss: 0.6208 - val_accuracy: 0.8043\n",
            "Epoch 11/20\n",
            "106/106 [==============================] - 17s 156ms/step - loss: 0.2674 - accuracy: 0.9055 - val_loss: 0.6273 - val_accuracy: 0.7757\n",
            "Epoch 12/20\n",
            "106/106 [==============================] - 17s 156ms/step - loss: 0.2280 - accuracy: 0.9212 - val_loss: 0.6551 - val_accuracy: 0.7733\n",
            "Epoch 13/20\n",
            "106/106 [==============================] - 16s 155ms/step - loss: 0.1905 - accuracy: 0.9357 - val_loss: 0.7521 - val_accuracy: 0.7446\n",
            "Epoch 14/20\n",
            "106/106 [==============================] - 17s 156ms/step - loss: 0.1655 - accuracy: 0.9475 - val_loss: 0.7023 - val_accuracy: 0.7828\n",
            "Epoch 15/20\n",
            "106/106 [==============================] - 17s 157ms/step - loss: 0.1274 - accuracy: 0.9573 - val_loss: 0.7357 - val_accuracy: 0.7804\n",
            "Epoch 16/20\n",
            "106/106 [==============================] - 17s 157ms/step - loss: 0.1070 - accuracy: 0.9671 - val_loss: 0.7922 - val_accuracy: 0.8043\n",
            "Epoch 17/20\n",
            "106/106 [==============================] - 17s 156ms/step - loss: 0.0886 - accuracy: 0.9730 - val_loss: 0.7762 - val_accuracy: 0.7852\n",
            "Epoch 18/20\n",
            "106/106 [==============================] - 17s 158ms/step - loss: 0.0710 - accuracy: 0.9807 - val_loss: 0.8559 - val_accuracy: 0.7900\n",
            "Epoch 19/20\n",
            "106/106 [==============================] - 17s 157ms/step - loss: 0.0599 - accuracy: 0.9849 - val_loss: 0.9236 - val_accuracy: 0.7852\n",
            "Epoch 20/20\n",
            "106/106 [==============================] - 17s 157ms/step - loss: 0.0791 - accuracy: 0.9727 - val_loss: 0.9485 - val_accuracy: 0.7900\n",
            "14/14 [==============================] - 1s 35ms/step - loss: 0.9485 - accuracy: 0.7900\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "s6KqmP2NggPE",
        "outputId": "009fe25f-7b9c-421c-bd66-7b24661c007e"
      },
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "\n",
        "import keras\n",
        "from keras.models import Sequential\n",
        "from keras.utils import np_utils\n",
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "from keras.layers import Dense, Activation, Flatten, Dropout, BatchNormalization\n",
        "from keras.layers import Conv2D, MaxPooling2D\n",
        "from keras.datasets import cifar10\n",
        "from keras import regularizers, optimizers\n",
        "import numpy as np\n",
        "from matplotlib import pyplot\n",
        "\n",
        "\n",
        "\n",
        "num_classes = 7\n",
        "\n",
        "# number of hidden units variable \n",
        "# we are declaring this variable here and use it in our CONV layers to make it easier to update from one place\n",
        "base_hidden_units = 32\n",
        "\n",
        "# l2 regularization hyperparameter\n",
        "weight_decay = 1e-4 \n",
        "\n",
        "# instantiate an empty sequential model \n",
        "model = Sequential()\n",
        "\n",
        "# CONV1\n",
        "# notice that we defined the input_shape here because this is the first CONV layer. \n",
        "# we don’t need to do that for the remaining layers\n",
        "model.add(Conv2D(base_hidden_units, (3,3), padding='same', kernel_regularizer=regularizers.l2(weight_decay),input_shape=(48, 48, 1) ))\n",
        "model.add(Activation('relu'))\n",
        "model.add(BatchNormalization())\n",
        "\n",
        "# CONV2\n",
        "model.add(Conv2D(base_hidden_units, (3,3), padding='same', kernel_regularizer=regularizers.l2(weight_decay)))\n",
        "model.add(Activation('relu'))\n",
        "model.add(BatchNormalization())\n",
        "model.add(MaxPooling2D(pool_size=(2,2)))\n",
        "model.add(Dropout(0.2))\n",
        "\n",
        "# CONV3\n",
        "model.add(Conv2D(2*base_hidden_units, (3,3), padding='same', kernel_regularizer=regularizers.l2(weight_decay)))\n",
        "model.add(Activation('relu'))\n",
        "model.add(BatchNormalization())\n",
        "\n",
        "# CONV4\n",
        "model.add(Conv2D(2*base_hidden_units, (3,3), padding='same', kernel_regularizer=regularizers.l2(weight_decay)))\n",
        "model.add(Activation('relu'))\n",
        "model.add(BatchNormalization())\n",
        "model.add(MaxPooling2D(pool_size=(2,2)))\n",
        "model.add(Dropout(0.3))\n",
        "\n",
        "# CONV5\n",
        "model.add(Conv2D(4*base_hidden_units, (3,3), padding='same', kernel_regularizer=regularizers.l2(weight_decay)))\n",
        "model.add(Activation('relu'))\n",
        "model.add(BatchNormalization())\n",
        "\n",
        "# CONV6\n",
        "model.add(Conv2D(4*base_hidden_units, (3,3), padding='same', kernel_regularizer=regularizers.l2(weight_decay)))\n",
        "model.add(Activation('relu'))\n",
        "model.add(BatchNormalization())\n",
        "model.add(MaxPooling2D(pool_size=(2,2)))\n",
        "model.add(Dropout(0.4))\n",
        "\n",
        "# FC7\n",
        "model.add(Flatten())\n",
        "model.add(Dense(num_classes, activation='softmax'))\n",
        "\n",
        "# print model summary\n",
        "model.summary()\n",
        "\n",
        "opt = tf.keras.optimizers.RMSprop(learning_rate=0.0003, decay=1e-6)\n",
        "\n",
        "model.compile(optimizer=opt, loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "batch_size = 64\n",
        "epochs=125\n",
        "\n",
        "history = model.fit(X_train, Y_train, validation_data=(X_valid, Y_valid), epochs=epochs)"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential_3\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " conv2d_10 (Conv2D)          (None, 48, 48, 32)        320       \n",
            "                                                                 \n",
            " activation_6 (Activation)   (None, 48, 48, 32)        0         \n",
            "                                                                 \n",
            " batch_normalization_6 (Batc  (None, 48, 48, 32)       128       \n",
            " hNormalization)                                                 \n",
            "                                                                 \n",
            " conv2d_11 (Conv2D)          (None, 48, 48, 32)        9248      \n",
            "                                                                 \n",
            " activation_7 (Activation)   (None, 48, 48, 32)        0         \n",
            "                                                                 \n",
            " batch_normalization_7 (Batc  (None, 48, 48, 32)       128       \n",
            " hNormalization)                                                 \n",
            "                                                                 \n",
            " max_pooling2d_7 (MaxPooling  (None, 24, 24, 32)       0         \n",
            " 2D)                                                             \n",
            "                                                                 \n",
            " dropout_3 (Dropout)         (None, 24, 24, 32)        0         \n",
            "                                                                 \n",
            " conv2d_12 (Conv2D)          (None, 24, 24, 64)        18496     \n",
            "                                                                 \n",
            " activation_8 (Activation)   (None, 24, 24, 64)        0         \n",
            "                                                                 \n",
            " batch_normalization_8 (Batc  (None, 24, 24, 64)       256       \n",
            " hNormalization)                                                 \n",
            "                                                                 \n",
            " conv2d_13 (Conv2D)          (None, 24, 24, 64)        36928     \n",
            "                                                                 \n",
            " activation_9 (Activation)   (None, 24, 24, 64)        0         \n",
            "                                                                 \n",
            " batch_normalization_9 (Batc  (None, 24, 24, 64)       256       \n",
            " hNormalization)                                                 \n",
            "                                                                 \n",
            " max_pooling2d_8 (MaxPooling  (None, 12, 12, 64)       0         \n",
            " 2D)                                                             \n",
            "                                                                 \n",
            " dropout_4 (Dropout)         (None, 12, 12, 64)        0         \n",
            "                                                                 \n",
            " conv2d_14 (Conv2D)          (None, 12, 12, 128)       73856     \n",
            "                                                                 \n",
            " activation_10 (Activation)  (None, 12, 12, 128)       0         \n",
            "                                                                 \n",
            " batch_normalization_10 (Bat  (None, 12, 12, 128)      512       \n",
            " chNormalization)                                                \n",
            "                                                                 \n",
            " conv2d_15 (Conv2D)          (None, 12, 12, 128)       147584    \n",
            "                                                                 \n",
            " activation_11 (Activation)  (None, 12, 12, 128)       0         \n",
            "                                                                 \n",
            " batch_normalization_11 (Bat  (None, 12, 12, 128)      512       \n",
            " chNormalization)                                                \n",
            "                                                                 \n",
            " max_pooling2d_9 (MaxPooling  (None, 6, 6, 128)        0         \n",
            " 2D)                                                             \n",
            "                                                                 \n",
            " dropout_5 (Dropout)         (None, 6, 6, 128)         0         \n",
            "                                                                 \n",
            " flatten_3 (Flatten)         (None, 4608)              0         \n",
            "                                                                 \n",
            " dense_5 (Dense)             (None, 7)                 32263     \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 320,487\n",
            "Trainable params: 319,591\n",
            "Non-trainable params: 896\n",
            "_________________________________________________________________\n",
            "Epoch 1/125\n",
            "106/106 [==============================] - 66s 607ms/step - loss: 2.5328 - accuracy: 0.3797 - val_loss: 2.1025 - val_accuracy: 0.2792\n",
            "Epoch 2/125\n",
            "106/106 [==============================] - 64s 603ms/step - loss: 1.6811 - accuracy: 0.5382 - val_loss: 2.6029 - val_accuracy: 0.2792\n",
            "Epoch 3/125\n",
            "106/106 [==============================] - 64s 600ms/step - loss: 1.4032 - accuracy: 0.6079 - val_loss: 2.9293 - val_accuracy: 0.2792\n",
            "Epoch 4/125\n",
            "106/106 [==============================] - 64s 602ms/step - loss: 1.1486 - accuracy: 0.6666 - val_loss: 2.3941 - val_accuracy: 0.3986\n",
            "Epoch 5/125\n",
            "106/106 [==============================] - 63s 597ms/step - loss: 1.0171 - accuracy: 0.7051 - val_loss: 1.6980 - val_accuracy: 0.5107\n",
            "Epoch 6/125\n",
            "106/106 [==============================] - 63s 599ms/step - loss: 0.8753 - accuracy: 0.7481 - val_loss: 0.7972 - val_accuracy: 0.7255\n",
            "Epoch 7/125\n",
            "106/106 [==============================] - 63s 599ms/step - loss: 0.8300 - accuracy: 0.7472 - val_loss: 0.7193 - val_accuracy: 0.7589\n",
            "Epoch 8/125\n",
            "106/106 [==============================] - 63s 597ms/step - loss: 0.7429 - accuracy: 0.7745 - val_loss: 0.7348 - val_accuracy: 0.7828\n",
            "Epoch 9/125\n",
            "106/106 [==============================] - 63s 594ms/step - loss: 0.6532 - accuracy: 0.7931 - val_loss: 0.8592 - val_accuracy: 0.7709\n",
            "Epoch 10/125\n",
            "106/106 [==============================] - 63s 598ms/step - loss: 0.6053 - accuracy: 0.8145 - val_loss: 0.5919 - val_accuracy: 0.8329\n",
            "Epoch 11/125\n",
            "106/106 [==============================] - 64s 602ms/step - loss: 0.5502 - accuracy: 0.8296 - val_loss: 0.6609 - val_accuracy: 0.8305\n",
            "Epoch 12/125\n",
            "106/106 [==============================] - 63s 595ms/step - loss: 0.5005 - accuracy: 0.8483 - val_loss: 0.6696 - val_accuracy: 0.8234\n",
            "Epoch 13/125\n",
            "106/106 [==============================] - 64s 601ms/step - loss: 0.4476 - accuracy: 0.8518 - val_loss: 0.6338 - val_accuracy: 0.8305\n",
            "Epoch 14/125\n",
            "106/106 [==============================] - 65s 612ms/step - loss: 0.4509 - accuracy: 0.8625 - val_loss: 0.6925 - val_accuracy: 0.8043\n",
            "Epoch 15/125\n",
            "106/106 [==============================] - 63s 598ms/step - loss: 0.3989 - accuracy: 0.8731 - val_loss: 0.6547 - val_accuracy: 0.8138\n",
            "Epoch 16/125\n",
            "106/106 [==============================] - 63s 596ms/step - loss: 0.3878 - accuracy: 0.8823 - val_loss: 0.5717 - val_accuracy: 0.8377\n",
            "Epoch 17/125\n",
            "106/106 [==============================] - 64s 601ms/step - loss: 0.3530 - accuracy: 0.8883 - val_loss: 0.6221 - val_accuracy: 0.8377\n",
            "Epoch 18/125\n",
            "106/106 [==============================] - 64s 603ms/step - loss: 0.3410 - accuracy: 0.8972 - val_loss: 0.5388 - val_accuracy: 0.8449\n",
            "Epoch 19/125\n",
            "106/106 [==============================] - 63s 597ms/step - loss: 0.3211 - accuracy: 0.9084 - val_loss: 0.5204 - val_accuracy: 0.8568\n",
            "Epoch 20/125\n",
            "106/106 [==============================] - 64s 602ms/step - loss: 0.2985 - accuracy: 0.9090 - val_loss: 0.7049 - val_accuracy: 0.7971\n",
            "Epoch 21/125\n",
            "106/106 [==============================] - 63s 596ms/step - loss: 0.2591 - accuracy: 0.9226 - val_loss: 0.7365 - val_accuracy: 0.8019\n",
            "Epoch 22/125\n",
            "106/106 [==============================] - 64s 604ms/step - loss: 0.2668 - accuracy: 0.9158 - val_loss: 0.7121 - val_accuracy: 0.8115\n",
            "Epoch 23/125\n",
            "106/106 [==============================] - 63s 599ms/step - loss: 0.2592 - accuracy: 0.9277 - val_loss: 0.5901 - val_accuracy: 0.8210\n",
            "Epoch 24/125\n",
            "106/106 [==============================] - 64s 600ms/step - loss: 0.2230 - accuracy: 0.9312 - val_loss: 0.5497 - val_accuracy: 0.8592\n",
            "Epoch 25/125\n",
            "106/106 [==============================] - 63s 596ms/step - loss: 0.1992 - accuracy: 0.9443 - val_loss: 0.6797 - val_accuracy: 0.8138\n",
            "Epoch 26/125\n",
            "106/106 [==============================] - 63s 597ms/step - loss: 0.2107 - accuracy: 0.9395 - val_loss: 0.5932 - val_accuracy: 0.8616\n",
            "Epoch 27/125\n",
            "106/106 [==============================] - 64s 599ms/step - loss: 0.1998 - accuracy: 0.9419 - val_loss: 0.6231 - val_accuracy: 0.8353\n",
            "Epoch 28/125\n",
            "106/106 [==============================] - 64s 605ms/step - loss: 0.1921 - accuracy: 0.9472 - val_loss: 0.6169 - val_accuracy: 0.8520\n",
            "Epoch 29/125\n",
            "106/106 [==============================] - 64s 600ms/step - loss: 0.1818 - accuracy: 0.9493 - val_loss: 0.6153 - val_accuracy: 0.8544\n",
            "Epoch 30/125\n",
            "106/106 [==============================] - 65s 612ms/step - loss: 0.1686 - accuracy: 0.9529 - val_loss: 0.6352 - val_accuracy: 0.8640\n",
            "Epoch 31/125\n",
            "106/106 [==============================] - 64s 607ms/step - loss: 0.1590 - accuracy: 0.9588 - val_loss: 0.7111 - val_accuracy: 0.8401\n",
            "Epoch 32/125\n",
            "106/106 [==============================] - 63s 596ms/step - loss: 0.1475 - accuracy: 0.9627 - val_loss: 0.6784 - val_accuracy: 0.8329\n",
            "Epoch 33/125\n",
            "106/106 [==============================] - 64s 603ms/step - loss: 0.1475 - accuracy: 0.9624 - val_loss: 0.6631 - val_accuracy: 0.8592\n",
            "Epoch 34/125\n",
            "106/106 [==============================] - 63s 598ms/step - loss: 0.1471 - accuracy: 0.9603 - val_loss: 0.6858 - val_accuracy: 0.8401\n",
            "Epoch 35/125\n",
            "106/106 [==============================] - 64s 601ms/step - loss: 0.1366 - accuracy: 0.9671 - val_loss: 0.6360 - val_accuracy: 0.8663\n",
            "Epoch 36/125\n",
            "106/106 [==============================] - 65s 614ms/step - loss: 0.1462 - accuracy: 0.9621 - val_loss: 0.7927 - val_accuracy: 0.8425\n",
            "Epoch 37/125\n",
            "106/106 [==============================] - 64s 600ms/step - loss: 0.1286 - accuracy: 0.9659 - val_loss: 0.6506 - val_accuracy: 0.8544\n",
            "Epoch 38/125\n",
            "106/106 [==============================] - 64s 600ms/step - loss: 0.1300 - accuracy: 0.9713 - val_loss: 0.6651 - val_accuracy: 0.8473\n",
            "Epoch 39/125\n",
            "106/106 [==============================] - 63s 598ms/step - loss: 0.1276 - accuracy: 0.9701 - val_loss: 0.6521 - val_accuracy: 0.8568\n",
            "Epoch 40/125\n",
            "106/106 [==============================] - 64s 604ms/step - loss: 0.1155 - accuracy: 0.9787 - val_loss: 0.7125 - val_accuracy: 0.8449\n",
            "Epoch 41/125\n",
            "106/106 [==============================] - 64s 602ms/step - loss: 0.1081 - accuracy: 0.9775 - val_loss: 0.6918 - val_accuracy: 0.8568\n",
            "Epoch 42/125\n",
            "106/106 [==============================] - 64s 604ms/step - loss: 0.1235 - accuracy: 0.9724 - val_loss: 0.6554 - val_accuracy: 0.8544\n",
            "Epoch 43/125\n",
            "106/106 [==============================] - 65s 613ms/step - loss: 0.1054 - accuracy: 0.9781 - val_loss: 0.6607 - val_accuracy: 0.8520\n",
            "Epoch 44/125\n",
            "106/106 [==============================] - 64s 606ms/step - loss: 0.1151 - accuracy: 0.9787 - val_loss: 0.7184 - val_accuracy: 0.8401\n",
            "Epoch 45/125\n",
            "106/106 [==============================] - 64s 606ms/step - loss: 0.1007 - accuracy: 0.9793 - val_loss: 0.7130 - val_accuracy: 0.8473\n",
            "Epoch 46/125\n",
            "106/106 [==============================] - 65s 610ms/step - loss: 0.1170 - accuracy: 0.9772 - val_loss: 0.6638 - val_accuracy: 0.8592\n",
            "Epoch 47/125\n",
            "106/106 [==============================] - 65s 611ms/step - loss: 0.1063 - accuracy: 0.9772 - val_loss: 0.7001 - val_accuracy: 0.8473\n",
            "Epoch 48/125\n",
            "106/106 [==============================] - 65s 615ms/step - loss: 0.0996 - accuracy: 0.9804 - val_loss: 0.6858 - val_accuracy: 0.8640\n",
            "Epoch 49/125\n",
            "106/106 [==============================] - 64s 605ms/step - loss: 0.1045 - accuracy: 0.9793 - val_loss: 0.7535 - val_accuracy: 0.8234\n",
            "Epoch 50/125\n",
            "106/106 [==============================] - 65s 609ms/step - loss: 0.1090 - accuracy: 0.9772 - val_loss: 0.8052 - val_accuracy: 0.8544\n",
            "Epoch 51/125\n",
            "106/106 [==============================] - 65s 609ms/step - loss: 0.1089 - accuracy: 0.9804 - val_loss: 0.8032 - val_accuracy: 0.8496\n",
            "Epoch 52/125\n",
            "106/106 [==============================] - 64s 607ms/step - loss: 0.0859 - accuracy: 0.9864 - val_loss: 0.9401 - val_accuracy: 0.8353\n",
            "Epoch 53/125\n",
            "106/106 [==============================] - 65s 609ms/step - loss: 0.0947 - accuracy: 0.9840 - val_loss: 0.7733 - val_accuracy: 0.8663\n",
            "Epoch 54/125\n",
            "106/106 [==============================] - 64s 607ms/step - loss: 0.0973 - accuracy: 0.9819 - val_loss: 0.7253 - val_accuracy: 0.8568\n",
            "Epoch 55/125\n",
            "106/106 [==============================] - 64s 603ms/step - loss: 0.0831 - accuracy: 0.9867 - val_loss: 0.8606 - val_accuracy: 0.8544\n",
            "Epoch 56/125\n",
            "106/106 [==============================] - 64s 605ms/step - loss: 0.0892 - accuracy: 0.9822 - val_loss: 0.8167 - val_accuracy: 0.8640\n",
            "Epoch 57/125\n",
            "106/106 [==============================] - 64s 606ms/step - loss: 0.0876 - accuracy: 0.9864 - val_loss: 0.8346 - val_accuracy: 0.8592\n",
            "Epoch 58/125\n",
            "106/106 [==============================] - 65s 611ms/step - loss: 0.0973 - accuracy: 0.9807 - val_loss: 0.7044 - val_accuracy: 0.8473\n",
            "Epoch 59/125\n",
            "106/106 [==============================] - 64s 607ms/step - loss: 0.0904 - accuracy: 0.9849 - val_loss: 0.7814 - val_accuracy: 0.8425\n",
            "Epoch 60/125\n",
            "106/106 [==============================] - 64s 608ms/step - loss: 0.0884 - accuracy: 0.9840 - val_loss: 0.7520 - val_accuracy: 0.8592\n",
            "Epoch 61/125\n",
            "106/106 [==============================] - 65s 609ms/step - loss: 0.0830 - accuracy: 0.9870 - val_loss: 0.7995 - val_accuracy: 0.8568\n",
            "Epoch 62/125\n",
            "106/106 [==============================] - 65s 614ms/step - loss: 0.0878 - accuracy: 0.9867 - val_loss: 0.7696 - val_accuracy: 0.8568\n",
            "Epoch 63/125\n",
            "106/106 [==============================] - 65s 613ms/step - loss: 0.0785 - accuracy: 0.9896 - val_loss: 0.7876 - val_accuracy: 0.8544\n",
            "Epoch 64/125\n",
            "106/106 [==============================] - 64s 608ms/step - loss: 0.0923 - accuracy: 0.9876 - val_loss: 0.7884 - val_accuracy: 0.8520\n",
            "Epoch 65/125\n",
            "106/106 [==============================] - 64s 607ms/step - loss: 0.0816 - accuracy: 0.9870 - val_loss: 0.7279 - val_accuracy: 0.8449\n",
            "Epoch 66/125\n",
            "106/106 [==============================] - 64s 608ms/step - loss: 0.0849 - accuracy: 0.9849 - val_loss: 0.7747 - val_accuracy: 0.8496\n",
            "Epoch 67/125\n",
            "106/106 [==============================] - 64s 604ms/step - loss: 0.0833 - accuracy: 0.9861 - val_loss: 0.8901 - val_accuracy: 0.8496\n",
            "Epoch 68/125\n",
            "106/106 [==============================] - 64s 606ms/step - loss: 0.0767 - accuracy: 0.9884 - val_loss: 0.7420 - val_accuracy: 0.8592\n",
            "Epoch 69/125\n",
            "106/106 [==============================] - 64s 604ms/step - loss: 0.0833 - accuracy: 0.9864 - val_loss: 0.7445 - val_accuracy: 0.8687\n",
            "Epoch 70/125\n",
            "106/106 [==============================] - 64s 603ms/step - loss: 0.0797 - accuracy: 0.9881 - val_loss: 0.7542 - val_accuracy: 0.8663\n",
            "Epoch 71/125\n",
            "106/106 [==============================] - 64s 606ms/step - loss: 0.0767 - accuracy: 0.9887 - val_loss: 0.7244 - val_accuracy: 0.8735\n",
            "Epoch 72/125\n",
            "106/106 [==============================] - 65s 610ms/step - loss: 0.0850 - accuracy: 0.9876 - val_loss: 0.7009 - val_accuracy: 0.8783\n",
            "Epoch 73/125\n",
            "106/106 [==============================] - 64s 607ms/step - loss: 0.0798 - accuracy: 0.9884 - val_loss: 0.7334 - val_accuracy: 0.8663\n",
            "Epoch 74/125\n",
            "106/106 [==============================] - 64s 605ms/step - loss: 0.0823 - accuracy: 0.9881 - val_loss: 0.6834 - val_accuracy: 0.8759\n",
            "Epoch 75/125\n",
            "106/106 [==============================] - 64s 606ms/step - loss: 0.0805 - accuracy: 0.9902 - val_loss: 0.7061 - val_accuracy: 0.8878\n",
            "Epoch 76/125\n",
            "106/106 [==============================] - 64s 606ms/step - loss: 0.0735 - accuracy: 0.9887 - val_loss: 0.8627 - val_accuracy: 0.8544\n",
            "Epoch 77/125\n",
            "106/106 [==============================] - 64s 603ms/step - loss: 0.0766 - accuracy: 0.9890 - val_loss: 0.9066 - val_accuracy: 0.8592\n",
            "Epoch 78/125\n",
            "106/106 [==============================] - 64s 605ms/step - loss: 0.0783 - accuracy: 0.9878 - val_loss: 0.9177 - val_accuracy: 0.8663\n",
            "Epoch 79/125\n",
            "106/106 [==============================] - 64s 602ms/step - loss: 0.0744 - accuracy: 0.9905 - val_loss: 0.7573 - val_accuracy: 0.8449\n",
            "Epoch 80/125\n",
            "106/106 [==============================] - 64s 603ms/step - loss: 0.0855 - accuracy: 0.9887 - val_loss: 0.8167 - val_accuracy: 0.8449\n",
            "Epoch 81/125\n",
            "106/106 [==============================] - 64s 602ms/step - loss: 0.0723 - accuracy: 0.9911 - val_loss: 0.7778 - val_accuracy: 0.8687\n",
            "Epoch 82/125\n",
            "106/106 [==============================] - 64s 606ms/step - loss: 0.0729 - accuracy: 0.9908 - val_loss: 0.8558 - val_accuracy: 0.8592\n",
            "Epoch 83/125\n",
            "106/106 [==============================] - 64s 606ms/step - loss: 0.0806 - accuracy: 0.9887 - val_loss: 0.7301 - val_accuracy: 0.8711\n",
            "Epoch 84/125\n",
            "106/106 [==============================] - 64s 601ms/step - loss: 0.0723 - accuracy: 0.9908 - val_loss: 0.7272 - val_accuracy: 0.8687\n",
            "Epoch 85/125\n",
            "106/106 [==============================] - 64s 607ms/step - loss: 0.0765 - accuracy: 0.9920 - val_loss: 0.9293 - val_accuracy: 0.8473\n",
            "Epoch 86/125\n",
            "106/106 [==============================] - 64s 602ms/step - loss: 0.0822 - accuracy: 0.9876 - val_loss: 0.8506 - val_accuracy: 0.8663\n",
            "Epoch 87/125\n",
            "106/106 [==============================] - 64s 607ms/step - loss: 0.0735 - accuracy: 0.9905 - val_loss: 0.8094 - val_accuracy: 0.8759\n",
            "Epoch 88/125\n",
            "106/106 [==============================] - 64s 599ms/step - loss: 0.0728 - accuracy: 0.9920 - val_loss: 0.8270 - val_accuracy: 0.8807\n",
            "Epoch 89/125\n",
            "106/106 [==============================] - 64s 603ms/step - loss: 0.0746 - accuracy: 0.9881 - val_loss: 0.8506 - val_accuracy: 0.8663\n",
            "Epoch 90/125\n",
            "106/106 [==============================] - 64s 605ms/step - loss: 0.0721 - accuracy: 0.9920 - val_loss: 0.8859 - val_accuracy: 0.8520\n",
            "Epoch 91/125\n",
            "106/106 [==============================] - 64s 606ms/step - loss: 0.0677 - accuracy: 0.9923 - val_loss: 0.8405 - val_accuracy: 0.8663\n",
            "Epoch 92/125\n",
            "106/106 [==============================] - 64s 605ms/step - loss: 0.0721 - accuracy: 0.9917 - val_loss: 0.7698 - val_accuracy: 0.8663\n",
            "Epoch 93/125\n",
            "106/106 [==============================] - 65s 610ms/step - loss: 0.0739 - accuracy: 0.9911 - val_loss: 0.7916 - val_accuracy: 0.8520\n",
            "Epoch 94/125\n",
            "106/106 [==============================] - 65s 610ms/step - loss: 0.0637 - accuracy: 0.9935 - val_loss: 0.8157 - val_accuracy: 0.8640\n",
            "Epoch 95/125\n",
            "106/106 [==============================] - 64s 607ms/step - loss: 0.0750 - accuracy: 0.9929 - val_loss: 0.7865 - val_accuracy: 0.8568\n",
            "Epoch 96/125\n",
            "106/106 [==============================] - 64s 609ms/step - loss: 0.0700 - accuracy: 0.9932 - val_loss: 0.7279 - val_accuracy: 0.8735\n",
            "Epoch 97/125\n",
            "106/106 [==============================] - 64s 606ms/step - loss: 0.0731 - accuracy: 0.9902 - val_loss: 0.8703 - val_accuracy: 0.8735\n",
            "Epoch 98/125\n",
            "106/106 [==============================] - 64s 605ms/step - loss: 0.0698 - accuracy: 0.9917 - val_loss: 0.8187 - val_accuracy: 0.8687\n",
            "Epoch 99/125\n",
            "106/106 [==============================] - 65s 612ms/step - loss: 0.0680 - accuracy: 0.9923 - val_loss: 0.7913 - val_accuracy: 0.8759\n",
            "Epoch 100/125\n",
            "106/106 [==============================] - 64s 604ms/step - loss: 0.0709 - accuracy: 0.9923 - val_loss: 0.9199 - val_accuracy: 0.8783\n",
            "Epoch 101/125\n",
            "106/106 [==============================] - 64s 601ms/step - loss: 0.0664 - accuracy: 0.9923 - val_loss: 0.7756 - val_accuracy: 0.8759\n",
            "Epoch 102/125\n",
            "106/106 [==============================] - 64s 602ms/step - loss: 0.0757 - accuracy: 0.9908 - val_loss: 0.7991 - val_accuracy: 0.8735\n",
            "Epoch 103/125\n",
            "106/106 [==============================] - 64s 604ms/step - loss: 0.0679 - accuracy: 0.9944 - val_loss: 0.8532 - val_accuracy: 0.8831\n",
            "Epoch 104/125\n",
            "106/106 [==============================] - 64s 607ms/step - loss: 0.0649 - accuracy: 0.9938 - val_loss: 0.8601 - val_accuracy: 0.8807\n",
            "Epoch 105/125\n",
            "106/106 [==============================] - 64s 606ms/step - loss: 0.0655 - accuracy: 0.9944 - val_loss: 0.9020 - val_accuracy: 0.8616\n",
            "Epoch 106/125\n",
            "106/106 [==============================] - 64s 606ms/step - loss: 0.0659 - accuracy: 0.9929 - val_loss: 0.8113 - val_accuracy: 0.8735\n",
            "Epoch 107/125\n",
            "106/106 [==============================] - 64s 602ms/step - loss: 0.0762 - accuracy: 0.9905 - val_loss: 0.9898 - val_accuracy: 0.8544\n",
            "Epoch 108/125\n",
            "106/106 [==============================] - 64s 605ms/step - loss: 0.0696 - accuracy: 0.9923 - val_loss: 1.0747 - val_accuracy: 0.8592\n",
            "Epoch 109/125\n",
            "106/106 [==============================] - 64s 601ms/step - loss: 0.0722 - accuracy: 0.9911 - val_loss: 0.8271 - val_accuracy: 0.8831\n",
            "Epoch 110/125\n",
            "106/106 [==============================] - 64s 603ms/step - loss: 0.0738 - accuracy: 0.9914 - val_loss: 0.8302 - val_accuracy: 0.8711\n",
            "Epoch 111/125\n",
            "106/106 [==============================] - 64s 603ms/step - loss: 0.0696 - accuracy: 0.9926 - val_loss: 0.8126 - val_accuracy: 0.8831\n",
            "Epoch 112/125\n",
            "106/106 [==============================] - 64s 604ms/step - loss: 0.0674 - accuracy: 0.9923 - val_loss: 0.7876 - val_accuracy: 0.8783\n",
            "Epoch 113/125\n",
            "106/106 [==============================] - 64s 608ms/step - loss: 0.0657 - accuracy: 0.9926 - val_loss: 0.8775 - val_accuracy: 0.8711\n",
            "Epoch 114/125\n",
            "106/106 [==============================] - 64s 604ms/step - loss: 0.0688 - accuracy: 0.9926 - val_loss: 0.8470 - val_accuracy: 0.8735\n",
            "Epoch 115/125\n",
            "106/106 [==============================] - 65s 615ms/step - loss: 0.0677 - accuracy: 0.9923 - val_loss: 0.9089 - val_accuracy: 0.8663\n",
            "Epoch 116/125\n",
            "106/106 [==============================] - 65s 615ms/step - loss: 0.0644 - accuracy: 0.9944 - val_loss: 0.9147 - val_accuracy: 0.8520\n",
            "Epoch 117/125\n",
            "106/106 [==============================] - 64s 607ms/step - loss: 0.0627 - accuracy: 0.9938 - val_loss: 0.8733 - val_accuracy: 0.8640\n",
            "Epoch 118/125\n",
            "106/106 [==============================] - 64s 606ms/step - loss: 0.0659 - accuracy: 0.9920 - val_loss: 0.8015 - val_accuracy: 0.8807\n",
            "Epoch 119/125\n",
            "106/106 [==============================] - 64s 605ms/step - loss: 0.0654 - accuracy: 0.9947 - val_loss: 0.8194 - val_accuracy: 0.8878\n",
            "Epoch 120/125\n",
            "106/106 [==============================] - 64s 603ms/step - loss: 0.0661 - accuracy: 0.9935 - val_loss: 0.9749 - val_accuracy: 0.8616\n",
            "Epoch 121/125\n",
            "106/106 [==============================] - 64s 607ms/step - loss: 0.0687 - accuracy: 0.9926 - val_loss: 0.7792 - val_accuracy: 0.8759\n",
            "Epoch 122/125\n",
            "106/106 [==============================] - 64s 607ms/step - loss: 0.0690 - accuracy: 0.9935 - val_loss: 0.8217 - val_accuracy: 0.8807\n",
            "Epoch 123/125\n",
            "106/106 [==============================] - 64s 605ms/step - loss: 0.0612 - accuracy: 0.9959 - val_loss: 0.9146 - val_accuracy: 0.8687\n",
            "Epoch 124/125\n",
            "106/106 [==============================] - 64s 607ms/step - loss: 0.0582 - accuracy: 0.9967 - val_loss: 0.9528 - val_accuracy: 0.8520\n",
            "Epoch 125/125\n",
            "106/106 [==============================] - 65s 609ms/step - loss: 0.0670 - accuracy: 0.9935 - val_loss: 0.7818 - val_accuracy: 0.8711\n"
          ]
        }
      ]
    }
  ]
}